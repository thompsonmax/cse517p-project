EMBED_DIM=512 # Dimension of embedding
N_HEADS=8 # Number of attention heads
N_DECODER_LAYERS=8 # Number of decoder layers
TRANS_LAYERS=3 # Number of transformer layers
FF_DIM=1024 # Dimension of FF 
DROPOUT_RATE=0.1
LR=0.001
NUM_EPOCHS=10
DATASET_MAX_SAMPLES=250
SEQ_LENGTH=64
BLOCK_SIZE=64
STEP_SIZE=256
BATCH_SIZE=64
EVAL_BATCH_SIZE=32
CHAR_VOCAB_SIZE=25000 # Low 20000s seems about right
UNICODE_MAX_CODE_POINT = 65535 # BMP
PADDING_CHAR_IDX = 0 # Padding character index
UNK_CHAR_IDX = 1 # Unknown character index
PADDING_CODE_POINT = 128 # Padding character
UNK_CODE_POINT = 0 # Unknown character
TOKENIZER_MODEL="xlm-roberta-base"