# VOCAB_SIZE=50000 # Number of words in embedding
# Results in a model with 635706865 params
EMBED_DIM=2048 # Dimension of embedding
N_HEADS=16 # Number of attention heads
N_DECODER_LAYERS=10 # Number of decoder layers
TRANS_LAYERS=3 # Number of transformer layers
FF_DIM=8192 # Dimension of FF 
DROPOUT_RATE=0.1
LR=1e-04
LR_MIN=1e-09
LR_T0=5
LR_T_MULT=1.0
LR_DECAY_PER_EPOCH=0.8
NUM_EPOCHS=15
DATASET_MAX_SAMPLES=50
SEQ_LENGTH=128
BLOCK_SIZE=128
STEP_SIZE=192
BATCH_SIZE=48
GRADIENT_CLIP=0.5
EVAL_BATCH_SIZE=32
EVAL_TOTAL_SIZE=512
CHAR_VOCAB_SIZE=25000 # Low 20000s seems about right
EPOCH_STEPS=30000
EVAL_STEPS=15
UNICODE_MAX_CODE_POINT = 65535 # BMP
PADDING_CHAR_IDX = 0 # Padding character index
UNK_CHAR_IDX = 1 # Unknown character index
PADDING_CODE_POINT = 128 # Padding character
UNK_CODE_POINT = 0 # Unknown character
TOKENIZER_MODEL="xlm-roberta-base"
TOKENIZER_MAX_LENGTH=512
RANDOM_SEED=132323487