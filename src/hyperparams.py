# VOCAB_SIZE=50000 # Number of words in embedding
EMBED_DIM=2048 # Dimension of embedding
N_HEADS=16 # Number of attention heads
N_DECODER_LAYERS=20 # Number of decoder layers
TRANS_LAYERS=3 # Number of transformer layers
FF_DIM=8192 # Dimension of FF 
DROPOUT_RATE=0.1
LR=0.001
LR_DECAY_PER_EPOCH=0.7
NUM_EPOCHS=10
DATASET_MAX_SAMPLES=250
SEQ_LENGTH=256
BLOCK_SIZE=256
STEP_SIZE=192
BATCH_SIZE=128
EVAL_BATCH_SIZE=32
EVAL_TOTAL_SIZE=512
CHAR_VOCAB_SIZE=25000 # Low 20000s seems about right
EPOCH_STEPS=250000
EVAL_STEPS=27
UNICODE_MAX_CODE_POINT = 65535 # BMP
PADDING_CHAR_IDX = 0 # Padding character index
UNK_CHAR_IDX = 1 # Unknown character index
PADDING_CODE_POINT = 128 # Padding character
UNK_CODE_POINT = 0 # Unknown character
TOKENIZER_MODEL="xlm-roberta-base"
TOKENIZER_MAX_LENGTH=512